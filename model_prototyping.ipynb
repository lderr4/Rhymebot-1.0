{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf58373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 14:17:07.025289: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import ClassLabel\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b57614e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"example_dataset/lyrics_dataset.csv\")\n",
    "pac = pd.read_csv(\"lyric_files/2pac.csv\", index_col=False)\n",
    "pac = pac.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b4a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodetoascii(text):\n",
    "    ascii_code = (text.replace('\\xe2\\x80\\x99', \"'\").\n",
    "            replace('\\xc3\\xa9', 'e').\n",
    "            replace('\\xe2\\x80\\x90', '-').\n",
    "            replace('\\xe2\\x80\\x91', '-').\n",
    "            replace('\\xe2\\x80\\x92', '-').\n",
    "            replace('\\xe2\\x80\\x93', '-').\n",
    "            replace('\\xe2\\x80\\x94', '-').\n",
    "            replace('\\xe2\\x80\\x94', '-').\n",
    "            replace('\\xe2\\x80\\x98', \"'\").\n",
    "            replace('\\xe2\\x80\\x9b', \"'\").\n",
    "            replace('\\xe2\\x80\\x9c', '\"').\n",
    "            replace('\\xe2\\x80\\x9c', '\"').\n",
    "            replace('\\xe2\\x80\\x9d', '\"').\n",
    "            replace('\\xe2\\x80\\x9e', '\"').\n",
    "            replace('\\xe2\\x80\\x9f', '\"').\n",
    "            replace('\\xe2\\x80\\xa6', '...').#\n",
    "            replace('\\xe2\\x80\\xb2', \"'\").\n",
    "            replace('\\xe2\\x80\\xb3', \"'\").\n",
    "            replace('\\xe2\\x80\\xb4', \"'\").\n",
    "            replace('\\xe2\\x80\\xb5', \"'\").\n",
    "            replace('\\xe2\\x80\\xb6', \"'\").\n",
    "            replace('\\xe2\\x80\\xb7', \"'\").\n",
    "            replace('\\xe2\\x81\\xba', \"+\").\n",
    "            replace('\\xe2\\x81\\xbb', \"-\").\n",
    "            replace('\\xe2\\x81\\xbc', \"=\").\n",
    "            replace('\\xe2\\x81\\xbd', \"(\").\n",
    "            replace('\\xe2\\x81\\xbe', \")\").\n",
    "            replace('\\n', \" \\n \").\n",
    "            replace('\\n \\n \\n ', \" \\n \\n \").\n",
    "            replace('\\n  \\n  \\n ', \" \\n \\n \").\n",
    "            replace('\\r', \"\").\n",
    "            strip('. ').\n",
    "            strip('\\n ')\n",
    "            )\n",
    "    return ascii_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb04a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Lyrics\"] = df['Lyrics'].apply(lambda x: unicodetoascii(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc1d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that cleans consecutive duplicated phrases and removes ad-libs from a list of strings\n",
    "\n",
    "def remove_dups_ad_libs(l):\n",
    "    \n",
    "    for i in range(len(l)):\n",
    "        phrase = l[i]\n",
    "        s_phrase = phrase.strip(' ')\n",
    "        if s_phrase:\n",
    "            if s_phrase[0]=='(' and s_phrase[-1]==')': # check if the phrase is an ad-lib and remove it\n",
    "                l[i]=\"\"\n",
    "        if i < len(l) - 2 and (s_phrase == l[i+1].strip(' ') or s_phrase == l[i+2].strip(' ')): # remove duplicated phrases and leave the last occurence\n",
    "            l[i]= \"\"\n",
    "    l = l[:-1]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c688200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying remove_dups_ad_libs function\n",
    "for row in range(df.shape[0]):\n",
    "    lyrics = df.iloc[row,2]\n",
    "    l = lyrics.split('\\n')\n",
    "    clean_l = remove_dups_ad_libs(l)\n",
    "    while True:\n",
    "        try:\n",
    "            clean_l.remove('')\n",
    "        except:\n",
    "            break\n",
    "    clean_lyrics = '\\n'.join(clean_l)\n",
    "    df.iloc[row,2] = clean_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9d0365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3614\n",
      "3614\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df.dropna(subset=['Lyrics'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb0bb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d6b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_Lyrics = train['Lyrics']\n",
    "df_train_Lyrics.to_csv('Train_rap_bot.csv', index=False)\n",
    "\n",
    "df_val_Lyrics = test['Lyrics']\n",
    "df_val_Lyrics.to_csv('Val_rap_bot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b6729b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/lucasderr/.cache/huggingface/datasets/csv/default-ba9de918dfc59e02/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d04057f31149b19c6a0ccd23f84c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809ddf6524d94d32b4e5bca04e84ab45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/lucasderr/.cache/huggingface/datasets/csv/default-ba9de918dfc59e02/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391c9097f9734b2e86cc67da5c0ce434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_train = './Train_rap_bot.csv'\n",
    "path_to_validation = './Val_rap_bot.csv'\n",
    "datasets = load_dataset(\"csv\", data_files={\"train\": path_to_train, \"validation\": path_to_validation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bd3ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f037d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(data):\n",
    "    tokens = tokenizer(data[\"Lyrics\"], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08bf1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "\n",
    "for lyric in datasets['train']:\n",
    "    try:\n",
    "        tokens = tokenizer(text=lyric['Lyrics'])\n",
    "        train_tokens.append(tokens)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "train_tokens = Dataset.from_pandas(pd.DataFrame(train_tokens))\n",
    "    \n",
    "    \n",
    "test_tokens = []\n",
    "\n",
    "for lyric in datasets['validation']:\n",
    "    try:\n",
    "        tokens = tokenizer(lyric[\"Lyrics\"])\n",
    "        test_tokens.append(tokens)\n",
    "    except:\n",
    "        continue\n",
    "test_tokens = Dataset.from_pandas(pd.DataFrame(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed786ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06a89436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6c04297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3429 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset_train = train_tokens.map(group_texts,batched=True,batch_size=8)\n",
    "lm_dataset_test = test_tokens.map(group_texts,batched=True,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc26b779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels'],\n",
       "    num_rows: 12406\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3f4c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df250cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilgpt2'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_new = model_name.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name_new}-freestyle-bot\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True,\n",
    "    num_train_epochs=7.0,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps = 100.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0214e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset_train,\n",
    "    eval_dataset=lm_dataset_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8ea43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
